---
title: "Homework 2"
author: "Weijia Qian"
header-includes: \usepackage{multirow}
output:
  pdf_document: default
  html_document: default
urlcolor: blue
---

```{r, include=FALSE}
library(gt)
library(here)
library(survival)
library(tidyverse)
knitr::opts_chunk$set(tidy = FALSE, echo = FALSE, warning = FALSE)
```

GitHub repository: https://github.com/wqian22/bios731_hw2_qian.git

### Problem 1: Newton's method

The logistic regression model assumes:
$$P(Y_i=1|X_i)=\pi_i=\frac{e^{X_i^T\beta}}{1+e^{X_i^T\beta}}$$
The likelihood function is:
$$L(\beta)=\prod_{i=1}^n \pi_i^{y_i}(1-\pi_i)^{1-y_i}$$
The log-likelihood is:
$$
\begin{aligned}
l(\beta) &= \sum_{i=1}^n\left[y_i\log(\pi_i)+(1-y_i)\log(1-\pi_i)\right]\\
&= \sum_{i=1}^n\left[y_iX_i^T\beta-\log(1+e^{X_i^T\beta})\right]
\end{aligned}
$$
Compute the gradient:
$$
\begin{aligned}
\frac{\partial\pi_i}{\partial\beta_j}&=\pi_i(1-\pi_i)x_{ij}\\
\frac{\partial l(\beta)}{\partial\beta_j}&=\sum_{i=1}^n\left[\frac{y_i}{\pi_i}\frac{\partial\pi_i}{\partial\beta_j}+\frac{1-y_i}{1-\pi_i}(-\frac{\partial\pi_i}{\partial\beta_j})\right]\\
&=\sum_{i=1}^n(y_i-\pi_i)x_{ij}
\end{aligned}
$$
In matrix form, defining $\underset{n\times p}{X}$ as the design matrix, $y=(y_1,\dots, y_n)^T$, $\pi=(\pi_1,\dots, \pi_n)^T$, the gradient vector is:
$$\Delta l(\beta)=X^T(y-\pi)$$
The Hessian matrix $\underset{p\times p}{H}$ is the second derivative of the log-likelihood:
$$H_{jk}=\frac{\partial^2l(\beta)}{\partial\beta_j\partial\beta_k}=-\sum_{i=1}^n\pi_i(1-\pi_i)x_{ij}x_{jk}$$
In matrix form, let $W$ be an $n\times n$ diagonal matrix with $\pi_i(1-\pi_i)$ on the diagonal:
$$H=-X^TWX$$

The Newton's method update for $\beta$ is:
$$
\begin{aligned}
\beta^{(t+1)}&=\beta^{(t)}-H^{-1}\Delta l(\beta)\\
&=\beta^{(t)}+(X^TWX)^{-1}X^T(y-\pi)
\end{aligned}
$$
Logistic regression a convex optimization problem. The Hessian of the log-likelihood is $H=-X^TWX$, where $W$ is a diagonal matrix with entries $W_{ii}=\pi_i(1-\pi_i)>0$, so W is positive definite and H is negative semi-definite. Thus, the log-likelihood is concave, meaning its negative (the loss function) is convex.


### Problem 2: MM

(A) In constructing a minorizing function, first prove the inequality

$$-\log\{1 + \exp{X_i^T\theta}\} \ge -\log\{1 + \exp(X_i^T\theta^{(k)})\} - \frac{\exp(X_i^T\theta) - \exp(X_i^T\theta^{(k)})}{1 + \exp(X_i^T\theta^{(k)})}$$
with equality when $\theta = \theta^{(k)}$. This eliminates the log terms.

Define $g(x)=-\log(1+e^x)$, then $g'(x)=-\frac{e^x}{1+e^x}$, $g''(x)=\frac{e^x}{(1+e^x)^2}>0$. Since $g$ is convex and differentiable, by the supporting hyperplane property,
$$g(X_i^T\theta)\ge g(X_i^T\theta^{(k)})+g'(X_i^T\theta^{(k)})(X_i^T\theta-X_i^T\theta^{(k)}), \quad \forall X_i^T\theta, X_i^T\theta^{(k)}\in \mathbb{R}$$
$$\Rightarrow -\log\{1 + \exp{X_i^T\theta}\} \ge -\log\{1 + \exp(X_i^T\theta^{(k)})\} - \frac{\exp(X_i^T\theta^{(k)})}{1 + \exp(X_i^T\theta^{(k)})}(X_i^T\theta-X_i^T\theta^{(k)})$$
Because $f(x)=e^x$ is convex and differentiable, by the supporting hyperplane property, $e^a\ge e^b+e^b(a-b)\Rightarrow e^a-e^b\ge e^b(a-b)$, $\forall a,b\in\mathbb{R}$. Let $a=X_i^T\theta$ and $b=X_i^T\theta^{(k)}$, we have
$$\exp(X_i^T\theta) - \exp(X_i^T\theta^{(k)}) \ge \exp(X_i^T\theta^{(k)})(X_i^T\theta-X_i^T\theta^{(k)}).$$
Thus,

$$-\log\{1 + \exp{X_i^T\theta}\} \ge -\log\{1 + \exp(X_i^T\theta^{(k)})\} - \frac{\exp(X_i^T\theta) - \exp(X_i^T\theta^{(k)})}{1 + \exp(X_i^T\theta^{(k)})}$$
with equality when $X_i^T\theta = X_i^T\theta^{(k)}\Leftrightarrow\theta=\theta^{(k)}$.

<br>

(B) Now apply the arithmetic-geometric mean inequality to the exponential function $\exp(X_i^T\theta)$ to separate the parameters. Assuming that $\theta$ has $p$ components and that there are $n$ observations, show that these maneuvers lead to a minorizing function

$$g(\theta|\theta^{(k)}) = -\frac{1}{p}\sum_{i = 1}^n \frac{\exp(X_i^T\theta^{(k)})}{1 + \exp(X_i^T\theta^{(k)})}\sum_{j = 1}^p\exp\{pX_{ij}(\theta_j-\theta_j^{(k)})\} + \sum_{i = 1}^nY_iX_i^T\theta = 0$$

up to a constant that does not depend on $\theta$.

By the arithmetic-geometric mean inequality,
$$
\begin{aligned}
\exp(X_i^T\theta)=\exp\left(\sum_{j=1}^pX_{ij}\theta_j\right)=\prod_{j=1}^p\exp(X_{ij}\theta_j)\le\frac{1}{p}\sum_{j=1}^p\exp(pX_{ij}\theta_j)
\end{aligned}
$$
Substitute this into the linearized term in (A):
$$
\begin{aligned}
- \frac{\exp(X_i^T\theta) - \exp(X_i^T\theta^{(k)})}{1 + \exp(X_i^T\theta^{(k)})} &\ge-\frac{\frac{1}{p}\sum_{j=1}^ppX_{ij}\theta_j-\exp(X_i^T\theta^{(k)})}{1 + \exp(X_i^T\theta^{(k)})}\\
&=-\frac{1}{p}\sum_{j=1}^p\frac{\exp(X_i^T\theta^{(k)})}{1 + \exp(X_i^T\theta^{(k)})}\exp\{pX_{ij}(\theta_j-\theta_j^{(k)})\}
\end{aligned}
$$
Summing over all observations $i$,
$$
\begin{aligned}
g(\theta|\theta^{(k)}) &= -\frac{1}{p}\sum_{i = 1}^n \frac{\exp(X_i^T\theta^{(k)})}{1 + \exp(X_i^T\theta^{(k)})}\sum_{j = 1}^p\exp\{pX_{ij}(\theta_j-\theta_j^{(k)})\} + \sum_{i = 1}^nY_iX_i^T\theta\\
&\le \sum_{i=1}^n - \frac{\exp(X_i^T\theta) - \exp(X_i^T\theta^{(k)})}{1 + \exp(X_i^T\theta^{(k)})}+ \sum_{i = 1}^nY_iX_i^T\theta\\
&\le \sum_{i=1}^n -\log\{1 + \exp{X_i^T\theta}\}+ \sum_{i = 1}^nY_iX_i^T\theta=l(\theta)
\end{aligned}
$$
up to a constant $-\log\{1 + \exp(X_i^T\theta^{(k)})\}$ that does not depend on $\theta$.

Therefore, $g(\theta|\theta^{(k)})$ is a minorizing function of $l(\theta)$.

(C) Finally, prove that maximizing $g(\theta|\theta^{(k)})$ consists of solving the equation

$$ -\sum_{i = 1}^n \frac{\exp(X_i^T\theta^{(k)})X_{ij}\exp(-pX_{ij}\theta_j^{(k)})}{1 + \exp(X_i^T\theta^{(k)})}\exp(pX_{ij}\theta_j) + \sum_{i = 1}^nY_iX_{ij} = 0$$
for each $j$.

To maximize $g(\theta|\theta^{(k)})$, we set $\frac{\partial g}{\partial \theta_j}=0$ for each $j$:
$$
\begin{aligned}
\frac{\partial g}{\partial \theta_j} &= -\frac{1}{p}\sum_{i = 1}^n\frac{\exp(X_i^T\theta^{(k)})}{1 + \exp(X_i^T\theta^{(k)})}\sum_{j=1}^p\frac{\partial}{\partial \theta_j}\exp\{pX_{ij}(\theta_j-\theta_j^{(k)})\}+\sum_{i = 1}^nY_iX_{ij}\\
&= -\frac{1}{p}\sum_{i = 1}^n\frac{\exp(X_i^T\theta^{(k)})}{1 + \exp(X_i^T\theta^{(k)})} pX_{ij}\exp\{pX_{ij}(\theta_j-\theta_j^{(k)})\}+\sum_{i = 1}^nY_iX_{ij}\\
&= -\sum_{i = 1}^n\frac{\exp(X_i^T\theta^{(k)})X_{ij}}{1 + \exp(X_i^T\theta^{(k)})} \exp\{pX_{ij}(\theta_j-\theta_j^{(k)})\}+\sum_{i = 1}^nY_iX_{ij}\\
&= -\sum_{i = 1}^n \frac{\exp(X_i^T\theta^{(k)})X_{ij}\exp(-pX_{ij}\theta_j^{(k)})}{1 + \exp(X_i^T\theta^{(k)})}\exp(pX_{ij}\theta_j) + \sum_{i = 1}^nY_iX_{ij} = 0
\end{aligned}
$$


## Extra credit (up to 10 points)! Expected vs. observed information


**Part A**: Show that the expected and observed information are equivalent for logistic regression

<br>
**Part B**: Let's say you are instead performing probit regression, which is similar to logistic regression but with a different link function.  Specifically, probit regression uses a probit link: 


$$\Phi^{-1}(Pr[Y_i = 1 |X_i]) = X_i^T\beta,$$

where $\Phi^{-1}$ is inverse of the CDF for the standard normal distribution.  **Are the expected and observed information equivalent for probit regression?** Justify why or why not.